训练部分数据：两列：第一列图像ID 第二列对应的文本 检索部分数据：六列：第一列图像ID 第二至第六列为对应的五个文本

运行步骤： 首先你需要你更改训练文件中的（最下面的几行）的数据集路径，模型路径在MOE_Model_CLIP中进行修改，就这两个修改对了那就可以运行 并且每个文件我们都加了一个测试方法，换完路径可以运行测试方法看一下能不能成~

如果想更换专家怎么办？ 请你在对应的TextEncoder和ImgaeEncoder中添加上该专家的信息，比如输出维度多少，目前文件中是统一输出到1024维度。 另外由于我们的镜像原因，难以从huggingface上引入模型，我们直接下载到本地后进行引用的，这一点需要注意！

有意思的现象： 你可能会问为什么我们文本专家只有两个？ 这个是很有趣的一件事，展开来说的话就是三个文本专家的话，选择的top-2的中总是没有Siglip/CLIP 但是我们尝试了一下用自注意力机制来挑选专家，发现正常的在三个专家下是可以选中的，但是效果没有两个专家的用特征路由的好 也可以用CLIP试一试

而且我们经过观察发现，除了Siglip/CLIP这种，一般参数量越大的模型越容易被选中，可以看到我们的配置文件，其中尝试了很多的专家，但一些比较弱的专家： gte这种，后期基本不会被选中。

另外这个实验复现的有条件的我们建议可以这样： 首先你将Siglip解冻一部分，我们的实验由于硬件限制，Siglip完全冰封，这一点也是我们的遗憾之一 另外你可以将更牛的模型引入进来，或者采用全参数训练，我们相信最后的结果一定很不错。

而且建议初始的时候先用Flickr30k数据集试试，因为 MSCOCO简直太大了！ 训练一次需要三天！！！ Flickr30k大概一天左右吧，勉勉强强可以接受~~~

代码方面可能感到疑惑的就是我们训练代码里面写了两阶段，但是我们实际只用了第一阶段，因为打算第一阶段只解冻投影，第二阶段解冻大部分参数，但是发现 完蛋，穷，运行不起来，诶，因此我们就没考虑第二阶段，但是我们认为这个第二阶段大概率是有效果的，因此我们将其留在了这里，等待有条件的有缘人去尝试！！！

我们认为这是个非常有意思的模型，代码的门槛也不高，会下载huggingface的模型就行，欢迎各位尝试！
